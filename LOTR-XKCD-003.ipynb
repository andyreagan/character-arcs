{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isbn = 9780618574940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "sys.path.append(\"/Users/andyreagan/tools/python\")\n",
    "from kitchentable.dogtoys import *\n",
    "from json import loads\n",
    "from re import findall,UNICODE\n",
    "from labMTsimple.labMTsimple.speedy import LabMT\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Book_class(object):\n",
    "    '''Book class to handle loading the calibre expanded epub format.\n",
    "    \n",
    "    Store all of the word lists, etc, in one place.'''\n",
    "    \n",
    "    def load_all(self,isbn):\n",
    "        self.files = listdir(\"data/kindle/kindle-015/\"+str(isbn))\n",
    "        self.txtfiles = listdir(\"data/kindle/kindle-015/\"+str(isbn)+\"/txt\")        \n",
    "\n",
    "        print(\"a sample of the text files:\")\n",
    "        print(self.txtfiles[:10])\n",
    "        f = open(\"data/kindle/kindle-015/\"+str(isbn)+\"/meta.json\",\"r\")\n",
    "        self.metadata = loads(f.read())\n",
    "        f.close()\n",
    "        print(\"this is the metadata:\")\n",
    "        print(self.metadata)\n",
    "        self.rawtext_by_chapter = []\n",
    "        for fname in self.txtfiles:\n",
    "            f = open(\"data/kindle/kindle-015/\"+str(isbn)+\"/txt/\"+fname,\"r\")\n",
    "            self.rawtext_by_chapter.append(f.read())\n",
    "            f.close()\n",
    "        self.word_lists_by_chapter = [listify(t) for t in self.rawtext_by_chapter]\n",
    "        self.chapter_ends = np.cumsum(list(map(len,self.word_lists_by_chapter)))\n",
    "        # add a 0 to the start, clip (to get the starts)\n",
    "        # could just move the above array around too...\n",
    "        self.chapter_beginnings = np.cumsum([0]+list(map(len,self.word_lists_by_chapter[:-1])))\n",
    "        self.chapter_centers = (self.chapter_ends+self.chapter_beginnings)/2\n",
    "#         print(list(map(len,self.word_lists_by_chapter)))\n",
    "#         print(self.chapter_ends)\n",
    "#         print(self.chapter_beginnings)\n",
    "#         print(self.chapter_centers)\n",
    "#         print(len(self.chapter_ends))\n",
    "#         print(len(self.word_lists_by_chapter))\n",
    "        \n",
    "    def chop(self,my_senti_dict,min_size=1000): #,save=False,outfile=\"\"):\n",
    "        \"\"\"Take long piece of text and generate the sentiment time series.\n",
    "\n",
    "        use save parameter to write timeseries to a file.\"\"\"\n",
    "        print(\"splitting the book into chunks of minimum size {}\".format(min_size))\n",
    "        \n",
    "        self.all_words = \" \".join(self.rawtext_by_chapter)\n",
    "        self.all_word_list = listify(self.all_words)\n",
    "        \n",
    "        self.all_fvec = []\n",
    "\n",
    "        for i in range(int(np.floor(len(self.all_word_list)/min_size))):\n",
    "            chunk = \"\"\n",
    "            if i == int(np.floor(len(self.all_word_list)/min_size))-1:\n",
    "                # take the rest\n",
    "                print('last chunk')\n",
    "                print('getting words ' + str(i*min_size) + ' through ' + str(len(self.all_word_list)-1))\n",
    "                for j in range(i*min_size,len(self.all_word_list)-1):\n",
    "                    chunk += self.all_word_list[j]+\" \"\n",
    "            else:\n",
    "                print('getting words ' + str(i*min_size) + ' through ' + str((i+1)*min_size))\n",
    "                for j in range(i*min_size,(i+1)*min_size):\n",
    "                    chunk += self.all_word_list[j]+\" \"\n",
    "                # print(chunk[0:10])\n",
    "\n",
    "            chunk_words = listify(chunk)\n",
    "            chunk_dict = dict()\n",
    "            for word in chunk_words:\n",
    "                if word in chunk_dict:\n",
    "                    chunk_dict[word] += 1\n",
    "                else:\n",
    "                    chunk_dict[word] = 1\n",
    "            text_fvec = my_senti_dict.wordVecify(chunk_dict)\n",
    "\n",
    "            # print chunk\n",
    "            # print 'the valence of {0} part {1} is {2}'.format(rawbook,i,textValence)\n",
    "\n",
    "            self.all_fvec.append(text_fvec)\n",
    "\n",
    "        return self.all_fvec\n",
    "    \n",
    "    def chopper_sliding(self,my_senti_dict,min_size=10000,num_points=100,stop_val=0.0,return_centers=False):\n",
    "        \"\"\"Take long piece of text and generate the sentiment time series.\n",
    "        We will now slide the window along, rather than make uniform pieces.\n",
    "\n",
    "        use save parameter to write timeseries to a file.\"\"\"\n",
    "\n",
    "        print(\"splitting the book into {} chunks of minimum size {}\".format(num_points,min_size))\n",
    "\n",
    "        # print(\"and printing those frequency vectors\"\n",
    "\n",
    "        # initialize timeseries, only thing we're after\n",
    "        timeseries = [0 for i in range(num_points)]\n",
    "        all_fvecs = [np.zeros(len(my_senti_dict.scorelist)) for i in range(num_points)]\n",
    "        window_centers = [0 for i in range(num_points)]\n",
    "\n",
    "        # how much to jump\n",
    "        # take one chunk out, and divide by the number of others we want (-1, the one we just took out)\n",
    "        # take the floor of this as the step, so we may take slightly smaller steps than possible\n",
    "        step = int(np.floor((len(self.all_word_list)-min_size)/(num_points-1)))\n",
    "        print(\"there are \"+str(len(self.all_word_list))+\" words in the book\")\n",
    "        print(\"step size \"+str(step))\n",
    "\n",
    "        # do it 99 times\n",
    "        for i in range(num_points-1):\n",
    "            window_centers[i] = i*step+(min_size)/2\n",
    "            # build the whole dict each time (could be a little better about this)\n",
    "            window_dict = dict()\n",
    "            # print(\"using words {} through {}\".format(i*step,min_size+i*step))\n",
    "            for word in self.all_word_list[(i*step):(min_size+i*step)]:\n",
    "                if word in window_dict:\n",
    "                    window_dict[word] += 1\n",
    "                else:\n",
    "                    window_dict[word] = 1\n",
    "            text_fvec = my_senti_dict.wordVecify(window_dict)\n",
    "            stoppedVec = stopper(text_fvec,my_senti_dict.scorelist,my_senti_dict.wordlist,stopVal=stop_val)\n",
    "            timeseries[i] = np.dot(my_senti_dict.scorelist,stoppedVec)/np.sum(stoppedVec)\n",
    "            all_fvecs[i] = text_fvec\n",
    "\n",
    "        # final chunk\n",
    "        i = num_points-1\n",
    "        window_centers[i] = i*step+(min_size)/2\n",
    "        # only difference: go to the end\n",
    "        # may be 10-100 more words there (we used floor on the step)\n",
    "        window_dict = dict()\n",
    "        # print(\"using words {} through {}\".format(i*step,len(all_words)))\n",
    "        for word in self.all_word_list[(i*step):]:\n",
    "            if word in window_dict:\n",
    "                window_dict[word] += 1\n",
    "            else:\n",
    "                window_dict[word] = 1\n",
    "        text_fvec = my_senti_dict.wordVecify(window_dict)\n",
    "        stoppedVec = stopper(text_fvec,my_senti_dict.scorelist,my_senti_dict.wordlist,stopVal=stop_val)\n",
    "        timeseries[i] = np.dot(my_senti_dict.scorelist,stoppedVec)/np.sum(stoppedVec)\n",
    "        all_fvecs[i] = text_fvec\n",
    "\n",
    "        if return_centers:\n",
    "            return timeseries,all_fvecs,window_centers\n",
    "\n",
    "        return timeseries,all_fvecs\n",
    "        # timeseries = coursegrain(timeseries,points=21)\n",
    "\n",
    "    #     g = open(outfile,\"w\")\n",
    "    #     g.write(\"{0:.0f}\".format(timeseries[0]))\n",
    "    #     for i in range(1,numPoints):\n",
    "    #         g.write(\",\")\n",
    "    #         g.write(\"{0:.0f}\".format(timeseries[i]))\n",
    "    #     g.write(\"\\n\")\n",
    "\n",
    "    def __init__(self,isbn):\n",
    "        self.isbn = isbn\n",
    "        self.load_all(isbn)\n",
    "        \n",
    "    def __str__(self):\n",
    "        if \"title\" in self.metadata:\n",
    "            return self.metadata[\"title\"]\n",
    "        else:\n",
    "            return \"Book (no title)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/kindle/kindle-015/9780618574940'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0c943200386d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_book\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBook_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misbn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_book\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmy_LabMT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabMT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopVal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_fvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_book\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_LabMT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9620ab986cc2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, isbn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0misbn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misbn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misbn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9620ab986cc2>\u001b[0m in \u001b[0;36mload_all\u001b[0;34m(self, isbn)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0misbn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/kindle/kindle-015/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misbn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxtfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/kindle/kindle-015/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misbn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/kindle/kindle-015/9780618574940'"
     ]
    }
   ],
   "source": [
    "my_book = Book_class(isbn)\n",
    "print(my_book)\n",
    "\n",
    "my_LabMT = LabMT(stopVal=0.0)\n",
    "all_fvecs = my_book.chop(my_LabMT)\n",
    "\n",
    "c = my_book.rawtext_by_chapter[11]\n",
    "w = my_book.word_lists_by_chapter[11]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(w[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "# Model not installed. Please run 'python -m spacy.en.download' to install latest compatible model.\n",
    "from spacy.parts_of_speech import NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.is_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e in doc.ents:\n",
    "    print(e,e.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.ents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(doc.ents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.ents[0].label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(doc.ents[0].root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.ents[0].root.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(doc.ents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_counts = dict()\n",
    "entity_counts_labels = dict()\n",
    "for ent in doc.ents:\n",
    "    if ent.string.rstrip() in entity_counts:\n",
    "        entity_counts[ent.string.rstrip()] += 1\n",
    "        entity_counts_labels[ent.string.rstrip()].append(ent.label_)\n",
    "    else:\n",
    "        entity_counts[ent.string.rstrip()] = 1\n",
    "        entity_counts_labels[ent.string.rstrip()] = [ent.label_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entity_counts_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten those counts?\n",
    "entity_root_counts = dict()\n",
    "for ent in doc.ents:\n",
    "    if ent.root.text in entity_root_counts:\n",
    "        entity_root_counts[ent.root.text] += 1\n",
    "    else:\n",
    "        entity_root_counts[ent.root.text] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entity_root_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entity_root_counts_flat = [(ent,entity_root_counts[ent]) for ent in entity_root_counts]\n",
    "entity_counts_flat = [(ent,entity_counts[ent]) for ent in entity_counts]\n",
    "def dictify(my_list):\n",
    "    a = dict()\n",
    "    for b in my_list:\n",
    "        if b in a:\n",
    "            a[b] += 1\n",
    "        else:\n",
    "            a[b] = 1\n",
    "    return a\n",
    "entity_counts_labels_flat = [(ent,len(entity_counts_labels[ent]),dictify(entity_counts_labels[ent])) for ent in entity_counts_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_root_counts_ents = [ent for ent in entity_root_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entity_root_counts_flat[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(entity_root_counts_flat,key=lambda name: name[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(entity_counts_flat,key=lambda name: name[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(entity_counts_labels_flat,key=lambda name: name[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the labels aren't helpful...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# store them across the whole book\n",
    "all_ents = dict()\n",
    "all_ents_roots = dict()\n",
    "all_ents_labels = dict()\n",
    "\n",
    "chapter_dicts = []\n",
    "for i,chap in enumerate(my_book.rawtext_by_chapter):\n",
    "    if len(my_book.word_lists_by_chapter[i]) > 1000:\n",
    "        print(\"-\"*80)\n",
    "        print(\"chapter {}\".format(i+1))\n",
    "        print(\"-\"*80)\n",
    "        doc = nlp(chap)\n",
    "        print(\"processing counts\")\n",
    "        entity_counts = dict()\n",
    "        for ent in doc.ents:\n",
    "            if ent.string.rstrip() in entity_counts:\n",
    "                entity_counts[ent.string.rstrip()] += 1\n",
    "            else:\n",
    "                entity_counts[ent.string.rstrip()] = 1\n",
    "            if ent.string.rstrip() in all_ents:\n",
    "                all_ents[ent.string.rstrip()] += 1\n",
    "                all_ents_labels[ent.string.rstrip()].append(ent.label_)\n",
    "            else:\n",
    "                all_ents[ent.string.rstrip()] = 1\n",
    "                all_ents_labels[ent.string.rstrip()] = [ent.label_]\n",
    "        entity_root_counts = dict()\n",
    "        for ent in doc.ents:\n",
    "            if ent.root.text.rstrip() in entity_root_counts:\n",
    "                entity_root_counts[ent.root.text.rstrip()] += 1\n",
    "            else:\n",
    "                entity_root_counts[ent.root.text.rstrip()] = 1\n",
    "            if ent.root.text.rstrip() in all_ents_roots:\n",
    "                all_ents_roots[ent.root.text.rstrip()] += 1\n",
    "            else:\n",
    "                all_ents_roots[ent.root.text.rstrip()] = 1\n",
    "        entity_root_counts_flat = [(ent,entity_root_counts[ent]) for ent in entity_root_counts]\n",
    "        entity_counts_flat = [(ent,entity_counts[ent]) for ent in entity_counts]\n",
    "        print(sorted(entity_root_counts_flat,key=lambda name: name[1],reverse=True)[:10])\n",
    "        print(sorted(entity_counts_flat,key=lambda name: name[1],reverse=True)[:10])\n",
    "        chapter_dicts.append(entity_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.ents[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.ents[0].lefts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.ents[0].rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.ents[0].subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.ents[0].orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.ents[0].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.ents[0].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.ents[0].label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_entity_root_counts_flat = [(ent,all_ents_roots[ent]) for ent in all_ents_roots]\n",
    "all_entity_counts_flat = [(ent,all_ents[ent]) for ent in all_ents]\n",
    "print(sorted(all_entity_root_counts_flat,key=lambda name: name[1],reverse=True)[:30])\n",
    "print(sorted(all_entity_counts_flat,key=lambda name: name[1],reverse=True)[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_ents_labels_flat = [[ent,len(all_ents_labels[ent]),dictify(all_ents_labels[ent])] for ent in all_ents_labels]\n",
    "all_ents_labels_flat_sorted = sorted(all_ents_labels_flat,key=lambda name: name[1],reverse=True)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_ents_labels_flat_sorted[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "people = []\n",
    "locations = []\n",
    "for ent in all_ents_labels_flat_sorted[:100]:\n",
    "    print(ent)\n",
    "    p_person = float(ent[2].get(\"PERSON\",0))/ent[1]\n",
    "    print(\"P(PERSON) = {0:.4f}\".format(p_person))\n",
    "    p_loc = float(ent[2].get(\"LOC\",0))/ent[1]\n",
    "    print(\"P(LOC) = {0:.4f}\".format(p_loc))\n",
    "    p_gpe = float(ent[2].get(\"GPE\",0))/ent[1]\n",
    "    if (p_loc > .01 or p_gpe > .2) and p_person < .5:\n",
    "        print(\"{} is a location\".format(ent[0]))\n",
    "        locations.append(ent[0])\n",
    "    elif p_person > 0.0:\n",
    "        print(\"{} is a person\".format(ent[0]))\n",
    "        people.append(ent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_ents_labels_flat_sorted[0][2].get(\"E\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people_probs = [(p,np.array([float(x.get(p,0)) for x in chapter_dicts])) for p in people]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "people_totals = np.zeros(len(chapter_dicts))\n",
    "for p in people_probs:\n",
    "    print(p[1])\n",
    "    people_totals += p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,8))\n",
    "plt.xkcd()\n",
    "for i in range(4):\n",
    "    plt.plot(people_probs[i][1]/people_totals,\"-s\",label=people[i])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location_probs = [(loc,np.array([float(x.get(loc,0)) for x in chapter_dicts])) for loc in locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc_totals = np.zeros(len(chapter_dicts))\n",
    "for p in location_probs:\n",
    "    loc_totals += p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,8))\n",
    "plt.xkcd()\n",
    "for i in range(len(locations)):\n",
    "    plt.plot(location_probs[i][1]/loc_totals,\"-s\",label=locations[i])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc_array = []\n",
    "for i in range(len(chapter_dicts)):\n",
    "    chap_locs = np.array([location_probs[j][1][i] for j in range(len(locations))])\n",
    "    winner = np.max(chap_locs)\n",
    "    loc = np.array(locations)[chap_locs == winner]\n",
    "    print(loc)\n",
    "    loc_array.append(loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build lists of the ranges of each\n",
    "breaks = [0]\n",
    "for i in range(len(chapter_dicts)-1):\n",
    "    if not loc_array[i] == loc_array[i+1]:\n",
    "        breaks.append(i+1)\n",
    "breaks.append(len(chapter_dicts))\n",
    "print(breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cont_locs = [loc_array[i] for i in breaks[:-1]]\n",
    "print(cont_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,8))\n",
    "for i in range(len(breaks)):\n",
    "    plt.plot([breaks[i],breaks[i]],[0,1],\"--\",color=\"0.7\")\n",
    "for i in range(len(breaks)-1):\n",
    "    print((breaks[i]+breaks[i+1])/2.)\n",
    "    plt.text((breaks[i]+breaks[i+1])/2.,.94,cont_locs[i],color=\"0.7\",horizontalalignment=\"center\",rotation=20)\n",
    "plt.xlim([breaks[0],breaks[-1]])\n",
    "print(len(chapter_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,8))\n",
    "for i in range(len(breaks)):\n",
    "    plt.plot([breaks[i],breaks[i]],[0,1],\"--\",color=\"0.7\")\n",
    "for i in range(len(breaks)-1):\n",
    "    plt.text((breaks[i]+breaks[i+1])/2.,.94,cont_locs[i],color=\"0.7\",horizontalalignment=\"center\",rotation=20)\n",
    "plt.xlim([breaks[0],breaks[-1]])\n",
    "for i in range(4):\n",
    "    plt.plot(np.arange(len(chapter_dicts))+0.5,people_probs[i][1]/people_totals,\"-s\",label=people[i])\n",
    "plt.legend(loc=\"center right\")\n",
    "mysavefig(\"getting-closer.png\",folder=\"media/figures\")\n",
    "mysavefig(\"getting-closer.pdf\",folder=\"media/figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
